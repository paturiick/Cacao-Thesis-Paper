\chapter{METHODOLOGY}


In this chapter, we detail the methodology employed to conduct the study, providing a comprehensive overview of the research design, data collection, and analytical procedures.

\section{Research Design}

\begin{figure}[H]
	\raggedright
	\textbf{Figure 3.1} \\ % Figure number left-aligned
	\textit{Modified Waterfall Model of SDLC} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/Waterfall.pdf} % image

	\vspace{0.5em}
	\raggedright

	\label{fig:waterfall}
\end{figure}

The methodology has been adopted from the Modified Waterfall Model of the Systems Development Life Cycle (SDLC). The methodology is selected due to its structured yet flexible nature, allowing for sequential phases with opportunities for feedback and refinement. This is particularly important in agricultural technology development, where both technical precision and field validation are critical. For this study, it enables the researchers to systematically design, implement, and evaluate the integration of UAV-collected imagery with YOLO for early disease detection in cacao pods, ensuring that each phase is thoroughly reviewed before progressing to the next, while still accommodating necessary adjustments. Figure 2 shows the stages necessary for development.

\section{Research Setting}

\begin{figure}[H]
	\raggedright
	\textbf{Figure 3.2} \\ % Figure number left-aligned
	\textit{The Cacao Farm in Claveria, Misamis Oriental} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/Cacao_Farm.pdf} % replace with actual image

	\vspace{0.5em}
	\raggedright

	\label{fig:cacao_farm}
\end{figure}

Figure 3.2 shows the cacao farm in Claveria, Misamis Oriental, which served as the research setting for this study.
The site was chosen because of its active cacao production and its relevance to the Philippine cacao industry, which continues to face significant challenges from black pod disease caused by \textit{Phytophthora palmivora}.
The farm provided an appropriate real-world environment for implementing and testing the UAV-based detection system, as it is representative of typical small- to medium-scale plantations in the region.

The farm environment contains multiple cacao trees arranged in rows, creating a suitable layout for structured UAV flights.
This arrangement allowed systematic data collection from varying altitudes and angles, ensuring that the captured images reflected diverse pod conditions, from healthy to infected.
The natural farm setting also exposed the system to real challenges such as fluctuating light, weather, and terrain conditions, which were essential for validating the practicality and robustness of the system.

Aside from the physical layout, the farm’s active cultivation and regular cacao pod growth cycles ensured a consistent availability of pods at different stages of maturity.
This was crucial for generating a dataset that included healthy pods, pods in early infection stages, and pods with advanced symptoms of black pod disease.
Farmers working in the area served as both observers and end-users, providing feedback on the practicality of UAV operations and the usability of the detection system.

The location in Claveria was also chosen because of its accessibility to researchers while still representing rural farming conditions where advanced monitoring systems are rarely deployed.
This balance made the site ideal for testing the feasibility of precision agriculture tools in smallholder farm contexts.
Ultimately, these factors made the cacao farm a practical and effective research setting for implementing and evaluating the proposed UAV-based cacao pod disease detection system.

\section{Research Setup}

The research setup utilized a UAV (Unmanned Aerial Vehicle) equipped with a built-in high-definition camera and a GPS module to conduct low-altitude monitoring of the cacao farm in Claveria, Misamis Oriental.
The UAV was flown at eye level, allowing it to capture detailed images of cacao pods directly from the side view of the trees.
This altitude ensured that the pods were recorded clearly, making it possible to identify early signs of black pod disease without the need for high-altitude aerial passes.

The UAV followed predefined paths along the rows of cacao trees to maintain systematic coverage of the plantation.
Images were taken at regular intervals while moving at a steady pace, ensuring consistent data collection.
Each image was automatically geotagged with precise coordinates provided by the integrated NEO-M8N GPS module.
This process allowed the researchers to later map infected pods within the farm using QGIS for visualization.

Since the UAV already came with a built-in imaging system, minimal hardware modifications were required.
The programmable flight commands ensured repeatability in data collection, making the dataset reliable for training and validating the YOLO-based detection model.

\begin{figure}[H]
	\raggedright
	\textbf{Figure 3.3} \\ % Figure number left-aligned
	\textit{Chosen UAV Flight Paths at Eye Level in the Cacao Farm} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/Uav_Flight_Paths.png} % replace with actual image

	\vspace{0.5em}
	\raggedright

	\label{fig:uav_flight_paths}
\end{figure}

Figure 3.3 presents the UAV’s selected flight paths at eye level across the cacao farm.
The drone moved parallel to the rows of cacao trees, ensuring overlapping coverage of pods and reducing blind spots.
Flying at eye level provided close-range, high-resolution imagery of cacao pods, enabling accurate detection of visible symptoms of \textit{Phytophthora palmivora}.
This low-altitude approach minimized issues related to lighting, wind, and stability, while ensuring that the captured data was both clear and consistent for system evaluation.




\section{Data Gathering}
To ensure a well-rounded and effective system design, data for this study will be collected from various relevant sources:
\subsection{Sources of Data}
\textbf{Cacao Farmers and Field Personnel}. Surveys and interviews will be conducted with cacao growers and farm workers to gather insights on existing practices for disease detection, common issues encountered in the field, and expectations for a UAV-based detection system. Agricultural Specialists. Input agricultural professionals will be obtained to identify key disease symptoms, validate detection criteria, and provide guidance on effective monitoring strategies for cacao pod health.

\textbf{Existing Literature and Research Studies}. A comprehensive review of academic and technical literature, such as studies by Baculio \& Barbosa (2022), Vera et al. (2024), and Solpot (2020), will support the design of the detection system by offering benchmarks on accuracy, image processing, and the use of machine learning in agriculture.

\textbf{Technology Experts}. Consultations with UAV technicians, AI developers, and computer vision experts will be sought to ensure the system’s technological components such as image acquisition and model training are both feasible and optimized for agricultural environments.

\subsection{Data Gathering Procedure}

To gather relevant data that will inform the design and development of the cacao pod disease detection system, this study will utilize both questionnaires and interviews.

\textbf{Questionnaire}. A structured questionnaire will be distributed to cacao farmers and field personnel to gather information about their current practices in detecting cacao pod diseases, the challenges they face in early identification, and their perspectives on using UAV-based solutions. Before distribution, the questionnaire will be reviewed and approved by the research adviser, agricultural specialists, and academic authorities to ensure that it is technically sound, ethically appropriate, and aligned with the objectives of the study.

\textbf{Interview}. Interviews will be conducted with cacao farmers, agricultural experts, plant pathologists, UAV technicians, and AI developers. These interviews aim to collect in-depth insights on disease symptoms, detection indicators, drone imaging strategies, and technical requirements for integrating YOLO into an agricultural setting.

\textbf{Existing Literature}. A review of relevant literature will be conducted to understand the current state of disease detection systems in agriculture, particularly focusing on cacao pod diseases. This will include studies on the use of UAVs, AI-driven disease detection models (such as YOLO), and the challenges associated with deploying such technologies in farming environments and provide a foundation for comparing the proposed system with existing solutions.

\subsection{Data Finding Analysis}

To gather relevant data, this study will employ multiple data collection methods.

\textbf{Qualitative Analysis}. Thematic analysis will be applied to interviews and open-ended survey responses. This approach will help identify recurring themes such as difficulties in manual disease detection, trust in machine learning techniques, and challenges related to the adoption of UAVs. Insights from this analysis will inform user-centered system design and guide improvements in usability and functionality.

\textbf{Quantitative Analysis}. Descriptive statistical analysis will be used for the structured survey data. This includes calculating frequencies, percentages, and average values to measure levels of technological readiness, prevalence of black pod disease, and the willingness of users to adopt UAV-based solutions for monitoring. These metrics will provide measurable indicators to support system feature prioritization.


\textbf{Feasibility Analysis}. Sensor specifications and machine learning model performance will be assessed through expert consultations and a review of relevant literature. This includes evaluating the accuracy of YOLO for disease detection and the practicality of drone operation in cacao farm environments using a performance matrix.

\textbf{Spatial Analysis}. Given that the UAV system captures geo-tagged images, spatial analysis will be conducted to examine the geographical distribution of detected disease cases. This analysis will help visualize infection hotspots across the farm and support precision intervention strategies. Tools such as heatmaps or geospatial clustering may be used to map and interpret disease spread over time and space.

\section{Requirement Gathering}

\subsection{User Definition}

Following the requirements gathering process, the primary user of the system has been identified.

\textit{Farmers} - The primary user of the system, responsible for utilizing UAVs to detect early signs of cacao pod diseases and making informed decisions for crop management.

\begin{longtable}{p{4cm} p{8cm}}
	\caption{System Requirements} \label{tab:sysreq}                                                                                                                                      \\

	\toprule
	\textbf{Category}        & \textbf{System Requirements}                                                                                                                               \\
	\midrule
	\endfirsthead

	\toprule
	\textbf{Category}        & \textbf{System Requirements}                                                                                                                               \\
	\midrule
	\endhead

	\bottomrule
	\endfoot

	Input Requirements       & - The system shall collect images of cacao pods captured by UAVs for disease detection.                                                                    \\
	                         & - The system shall allow users to initiate UAV image capture and review sessions through a simple interface.                                               \\
	                         & - The system shall utilize annotated image datasets for training the AI model, specifically focusing on black pod disease.                                 \\
	                         & - The system shall use GPS metadata from UAVs as input to geo-tag disease detection results.                                                               \\
	\midrule

	Process Requirements     & - The system shall process captured images using the YOLO model to identify and classify diseased cacao pods.                                              \\
	                         & - The system shall preprocess input images for consistency.                                                                                                \\
	                         & - The system shall utilize a labeled image dataset of cacao pods.                                                                                          \\
	                         & - The system shall include a manual annotation process where images are labeled with infection presence and location.                                      \\
	\midrule

	Output Requirements      & - The system shall display detection results, highlighting infected areas on cacao pods in real-time or after processing.                                  \\
	                         & - The system shall classify each detected pod as either healthy or infected.                                                                               \\
	                         & - The system shall provide GPS coordinates alongside detection results for mapping infected areas.                                                         \\
	                         & - The system shall generate summary reports including: total number of pods detected, number and percentage of infected pods, detection time and location. \\
	\midrule

	Control Requirements     & - The system shall implement secure access with user authentication to ensure that only authorized users (farmers) can access the system.                  \\
	                         & - The system shall maintain logs of image captures, analysis sessions, and user feedback for audit and traceability.                                       \\
	                         & - The system shall validate all image inputs and user entries to ensure accurate and usable data is processed.                                             \\
	\midrule

	Performance Requirements & - The system shall be capable of processing high resolution images in real-time or near real-time with minimal latency.                                    \\
	                         & - The system shall efficiently manage and store large datasets of images and detection results, supporting scalable usage over time.                       \\
	                         & - The system shall maintain uptime and availability to ensure uninterrupted use during farming operations.                                                 \\
\end{longtable}

\section{Design and Implementation}

\subsection{Context Level Diagram}

\begin{figure}[H]
	\raggedright
	\textbf{Figure 2} \\ % Figure number left-aligned
	\textit{Context Level Diagram of the Study} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/Context_Level.pdf} % image

	\vspace{0.5em}
	\raggedright

	\label{fig:context-level}
\end{figure}

Figure 2 illustrates the context-level diagram of the cacao disease detection system. This diagram provides a high-level overview of the interactions. As a context-level diagram, it abstracts away internal processing to focus solely on how external entities interact with the system as a whole. The system integrates an Unmanned Aerial Vehicle (UAV), which is responsible for capturing and transmitting aerial imagery of cacao pods. These images serve as critical input for the system’s disease detection processes. The Web Application functions as the user interface, facilitating communication between the system and its users. Through this platform, cacao farmers can input relevant field data and observations, while also receiving timely detection results. Cacao farmers contribute to the system by using the web app, submitting field data, and providing feedback for continuous improvement.

\subsection{System Architecture}

\begin{figure}[H]
	\raggedright
	\textbf{Figure 3} \\ % Figure number left-aligned
	\textit{System Architecture of the Study} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/Sys_Arch.pdf} % image

	\vspace{0.5em}
	\raggedright

	\label{fig:SysArch}
\end{figure}

Figure 3 illustrates the system architecture of the study. The process begins with the cacao trees, which are the central focus of the system. To monitor them, a proposed drone is deployed over the farm. This DJI RoboMaster TT is equipped with cameras and sensors that collect input data—primarily images and environmental readings—from the trees as it flies at the same level as them. The data captured by the drone is transmitted to an open-source controller with a camera module, which acts as the drone’s mini computer. This device processes the incoming images and sensor data. Additionally, a GPS module connected to the system sends precise geolocation data to the processor, tagging the collected images with accurate coordinates. Once the data is processed, it is temporarily stored in a memory unit. From there, the data is transmitted using a telemetry module, which serves as a bridge between the drone and the ground systems. The telemetry module sends the data wirelessly via RF to the web-based system, which acts as both a controller and a receiver. It receives the telemetry data and displays it in a user-friendly format, allowing the farmer to monitor the status of the cacao trees. The farmer can also send commands or control inputs back to the system using the system. All this information can be stored and retrieved from a database for later analysis or reference. The farmer is empowered with clear insights about the farm’s condition without needing to be physically present at each tree.


\subsection{Hardware and Software Requirements}

\begin{table}[H]
	\centering
	\caption{Hardware Requirements}
	\label{tab:hardreq}
	\begin{tabular}{ll}
		\toprule
		Drone      & DJI Robomaster TT (Tello Talent) \\
		\midrule
		GPS Module & NEO-M8N                          \\
		\bottomrule
	\end{tabular}
\end{table}

The hardware setup, summarized in Table 2, is composed of essential components that enable efficient data acquisition and geospatial referencing. The DJI RoboMaster TT (Tello Talent) drone is employed to capture aerial images of cacao pods, providing a stable and programmable platform for image collection. To ensure accurate geolocation tagging of each captured image, a NEO-M8N GPS module is integrated into the system. This GPS module delivers high-precision positional data, which enhances the reliability of spatial mapping and supports the alignment of imagery with corresponding field coordinates.

\begin{table}[H]
	\centering
	\caption{Software Requirements}
	\label{tab:softreq}
	\begin{tabular}{ll}
		\toprule
		Front-end               & Vue.js     \\
		\midrule
		Back-end                & Django     \\
		\midrule
		Database                & PostgreSQL \\
		\midrule
		API                     & QGIS       \\
		\midrule
		Web Server              & Nginx      \\
		\midrule
		Deep Learning Framework & YOLO       \\
		\bottomrule
	\end{tabular}
\end{table}

The software requirements, summarized in Table 3, specify the technologies utilized for interface development, system management, data storage, and disease detection. The front-end interface is built with Vue.js to enable user interaction and data visualization, while the back-end is powered by the Django framework, which manages system logic, workflows, and API integration. PostgreSQL functions as the primary database, storing user information, detection outputs, and geospatial metadata. Geospatial analysis and visualization of infected areas are facilitated through QGIS integration. The application is deployed using the Nginx web server to ensure efficient and reliable performance. For image-based disease detection, YOLO is implemented to process aerial imagery and identify signs of infection in cacao pods.

\subsection{Flowchart of Hardware and Software}

\begin{figure}[H]
	\raggedright
	\textbf{Figure 4} \\ % Figure number left-aligned
	\textit{Hardware Flowchart} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/Hard_Flow.pdf} % image

	\vspace{0.5em}
	\raggedright

pgrep -x zathura
	\label{fig:HardFlow}
\end{figure}

Figure 4 illustrates the operational workflow of the UAV system; it begins with powering on the UAV, followed by the initialization of its key sensors, including the camera, GPS module, and obstacle detection system. Once the sensors are active, the system checks for telemetry connectivity with the ground station; if the connection fails, the drone retries until successful. After establishing a link, the mission plan is loaded from the ground station, and the UAV proceeds to takeoff. During flight, two processes run in parallel: the drone continuously captures images and videos with GPS geotags for later processing, while simultaneously monitoring its environment for obstacles and adjusting its path when necessary. The system then checks whether the mission plan has been completed. If it is complete, the UAV proceeds to land; if not, it continues executing the remaining tasks until completion. Only after landing are the stored image and location data transmitted to the ground station for processing. The operation then concludes with system power down.

\begin{figure}[H]
	\raggedright
	\textbf{Figure 5} \\ % Figure number left-aligned
	\textit{Software Flowchart} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/Soft_Flow.pdf} % image

	\vspace{0.5em}
	\raggedright

	\label{fig:SoftFlow}
\end{figure}

Figure 5 shows the software workflow, starting with the user logging into the system and creating a mission plan that defines waypoints, altitude, and capture intervals. This plan is then sent to the UAV through SDK, after which the system awaits mission status updates. At this stage, a decision point determines whether the mission has been completed. If the mission is successful, the system retrieves the uploaded images with their GPS metadata; otherwise, it continues waiting until completion. Once the data is received, the images undergo preprocessing such as resizing, normalization, and augmentation before being passed through the YOLO algorithm for disease detection and classification. The detection results are then associated with GPS coordinates to ensure spatial accuracy and are mapped in QGIS to generate a clear visualization of affected areas. From this stage, two operations run in parallel: results are displayed on the dashboard for immediate user insight, while the same results are saved into the database for reporting and long-term storage. Finally, the processes converge, and the mission concludes to complete the workflow.

\subsection{Ground Station Architecture}

\begin{figure}[H]
	\raggedright
	\textbf{Figure 6} \\ % Figure number left-aligned
	\textit{Ground Station System Design and Workflow} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=1\textwidth]{figures/Ground_Station.pdf} % image 

	\vspace{0.5em}
	\raggedright

	\label{fig:GroundStation}
\end{figure}

Figure 6 presents the entire system that manages the processing of UAV data and delivers results to the end user. It is composed of two closely connected parts: the web application and the server components.

The web application, developed in Vue.js, serves as the main interface for users. It provides modules for user authentication, drone mission control, and the visualization of detection outputs. The dashboard summarizes key statistics such as the number of pods identified as healthy or infected, while the history module allows users to review past detections and track trends over time. Through this interface, farmers are able to easily monitor the health of their crops and make informed decisions.

The server side of the Ground Station integrates several core technologies. Nginx acts as the web server and reverse proxy, handling HTTPS traffic and routing requests. Django functions as the application server, implementing the system’s logic, managing authentication, and exposing a REST API for the web application. The detection process is carried out by the YOLO module, which receives aerial images and returns bounding boxes, labels, and confidence scores. Finally, PostgreSQL, extended with geospatial capabilities, stores the detection results along with their associated GPS coordinates. This allows the system to perform spatial queries and display disease distribution on a map.

The operational workflow begins when images captured by the UAV are transmitted to the Ground Station. Django submits the images to the YOLO detection module and then attaches geospatial information derived from the UAV’s GPS data. These enriched results are stored in PostgreSQL and subsequently retrieved by the web application for display.

\subsection{YOLO-Based Detection Implementation}
YOLO (You Only Look Once) algorithm, a CNN-based object detection framework that employs Convolutional Neural Networks to simultaneously determine the location of objects within an image and classify them. Unlike traditional image classification models, which merely indicate the presence of an object, YOLO predicts both what the object is and where it is situated by drawing bounding boxes around it in a single processing step.

\begin{figure}[H]
  \raggedright
  \textbf{Figure 7} \\ 
  \textit{Architecture of YOLO11, showing backbone with new C3K2 blocks, attention modules (C2PSA), Spatial Pyramid Pooling Fast (SPFF), and multi-scale detection heads (adapted from Khanam and Hussain, 2024)} 

  \vspace{0.5em}
  \centering
  \includegraphics[width=1\textwidth]{figures/yolov11.pdf}  

  \vspace{0.5em}
  \raggedright

  \label{fig:yolov11_architecture}
\end{figure}


YOLOv11 is employed since it introduces architectural improvements in feature extraction, particularly for small-object detection, and achieves higher accuracy with fewer parameters compared to earlier versions. YOLO11 also provides scalable model variants that allow researchers to balance accuracy and computational requirements, making it highly suited for UAV-captured aerial imagery where cacao pods may appear small, numerous, and partially obscured. Figure 7 shows the  backbone for feature extraction, the neck for aggregating multi-scale feature maps, and the detection head outputs used for classifying and locating objects at various scales.

\subsection{Dataset Collection Method}
The dataset for this study will consist of both a custom dataset and external sources to ensure diversity and robustness. A custom dataset will be generated using images captured by a DJI RoboMaster TT’s built-in camera during scheduled flights over cacao plantations. Capturing images from multiple angles and altitudes will enhance the comprehensiveness and reliability of the dataset. In addition, publicly available image datasets such as those from Kaggle will be incorporated to supplement the collected data and increase variability. To ensure the model performs well in diverse real-world conditions, the combined dataset will include images of both healthy and diseased cacao pods under different lighting conditions, weather scenarios, and backgrounds. Special attention will be given to capturing multiple stages of infection and varying degrees of pod visibility. All images will be manually annotated to label regions of interest, which is essential for the supervised training of the YOLO model.

\subsection{Dataset Preprocessing}

Before training and detection, the collected dataset will undergo a series of preprocessing steps to ensure consistency, quality, and suitability for the YOLOv11 model. First, UAV-captured videos are converted into individual frames at fixed intervals, with blurry or otherwise low-quality frames removed to reduce noise. Images, including those from external sources such as Kaggle, will be resized in accordance with YOLOv11's \texttt{imgsz} parameter to maintain a consistent input dimension while preserving the aspect ratio. Pixel values will be normalized (e.g., scaled to a [0,1] range or adjusted using mean and standard deviation) to aid convergence during training. 

The dataset will be divided into training, validation, and testing subsets (commonly 70\% / 20\% / 10\%), ensuring that class distributions (healthy vs. diseased) are preserved and preventing data leakage. Data augmentation will be applied to the training portion only, using transformations such as random flips, rotations, brightness/contrast adjustments, and possibly random crops, to enhance the model’s robustness to real-world variability. Annotation files will be standardized to a YOLO-compatible format so that both custom and external images follow the same label scheme.

\subsection{Cacao and Disease Detection}

The detection of cacao pods and the identification of black pod disease were implemented using the YOLOv11 algorithm. The UAV-captured images were processed by YOLOv11, which divided each image into a grid structure where every grid cell predicted bounding boxes, confidence scores, and class probabilities. This allowed the system to detect multiple cacao pods within a single frame and to determine their positions in the aerial imagery. Each detected pod was outlined with a bounding box, ensuring that the system marked its exact location even when the pods were partially obscured by foliage or affected by varying lighting conditions.

After the pods were detected, the system classified each one as either healthy or infected with black pod disease (\textit{Phytophthora palmivora}). The classification was based on visual features learned during training, including surface discoloration, dark lesions, and irregular textures characteristic of infected pods. YOLOv11 annotated the results directly on the processed images, producing outputs where healthy pods were labeled separately from diseased pods.


\section{UAV Drone System}

\begin{figure}[H]
	\raggedright
	\textbf{Figure 8} \\ % Auto-incremented figure number
	\textit{The DJI RoboMaster TT UAV} % Title left-aligned

	\vspace{0.5em}
	\centering
	\includegraphics[width=0.6\textwidth]{figures/Robomaster_TT.pdf} % replace with actual image

	\vspace{0.5em}
	\raggedright

	\label{fig:robomaster_tt}
\end{figure}

The DJI RoboMaster TT (Tello Talent), a programmable quadcopter
equipped with a Vision Positioning System, a 5 MP camera, and an expansion kit for modular extensions.
It was chosen for its lightweight design, stability at low altitudes, and compatibility with open-source
programming platforms.

The RoboMaster TT integrates several important functions. Its flight system combines a flight controller
and propulsion motors with the Vision Positioning System (VPS), which uses a downward-facing camera and
infrared sensors to maintain stable hovering at low altitudes without relying on GPS. This feature is
essential for operating indoors or close to the ground, such as at eye level within cacao plantations.
The onboard 5 MP camera enables the capture of still images and 720p HD video, suitable for documenting
cacao pods. Power is supplied by a 3.8 V, 1100 mAh LiPo battery, supporting flight times of up to
13 minutes. Safety features include propeller guards, battery protection, and automatic landing in case
of weak signals or low power. The expansion kit includes an ESP32-based open-source controller, providing
UART, I2C, GPIO, PWM, and SPI interfaces for integrating additional modules such as the GPS receiver.

\subsection{Command-based Mission Control with RoboMaster TT SDK}
The Software Development Kit (SDK) enables command-based mission planning with the
DJI RoboMaster TT. Through the SDK, the drone executes scripted flight commands such as
\texttt{takeoff}, \texttt{forward(x)}, \texttt{back(x)}, \texttt{cw(angle)}, and \texttt{land},
allowing it to follow predefined flight paths across cacao rows. During these missions, the drone
captures images of cacao pods at specific intervals, paired with location data provided by the
integrated NEO-M8N GPS module.

An offline detection approach was adopted: the collected images and GPS coordinates are stored
and later processed using the YOLO algorithm. The results of detection are then mapped in QGIS
to identify and visualize areas affected by \textit{Phytophthora palmivora}. This workflow
synchronizes drone navigation, image acquisition, and geotagging while accounting for the
hardware limitations of the RoboMaster TT.

\subsection{GPS Integration and Data Handling}
A NEO-M8N GPS module was connected to the open-source controller via UART for geolocation.
The NEO-M8N is a high-performance GNSS receiver capable of tracking multiple satellite systems
(GPS, GLONASS, Galileo, and BeiDou). It provides accurate position, velocity, and time data,
which were synchronized with each captured image to create geotagged datasets. This integration
enabled the precise mapping of cacao pods and potential disease symptoms in QGIS.

All communications between the UAV and the ground station were established through a
Wi-Fi connection. The SDK relies on UDP packets transmitted over Wi-Fi to send
flight commands and receive status updates. For structured telemetry such as GPS coordinates
and mission logs, the MQTT protocol was integrated as a lightweight messaging layer,
ensuring reliable delivery of small data packets. Image files, due to their larger size, were
transferred directly via the Wi-Fi link to the ground station.

After each mission, the drone transmitted images and GPS logs to a central database.
The ground station retrieved these records from the database for preprocessing
and analysis. This workflow ensured that raw images and geospatial metadata were securely
stored and easily accessible for the YOLO-based detection system and subsequent
spatial visualization in QGIS.


\subsection{Materials and Cost}

\begin{table}[H]
	\centering
	\caption{Components and Cost}
	\label{tab:components}
	\begin{tabular}{ll}
		\toprule
		Component                 & Price      \\
		\midrule
		Drone (DJI RoboMaster TT) & ₱20,000.00 \\
		\midrule
		NEO-M8N GPS module        & ₱300.00    \\
		\midrule
		Total Cost                & ₱20,300.00 \\
		\bottomrule
	\end{tabular}
\end{table}

\section{Testing}
Testing was conducted to ensure that the system operates accurately, efficiently, and is suitable for real-world use in cacao farm management. The process involved functional testing, usability testing, and performance testing. Functional testing verified whether key features—such as UAV image capture, detection of Phytophthora palmivora-infected cacao pods using the YOLO, and geotagging of infected trees—performed as intended, with each function tested through defined inputs and expected outputs. Usability testing assessed how easily end users, particularly cacao farmers, could navigate and interact with the system by performing essential tasks such as logging in, accessing detection results, and interpreting geotagged maps. Feedback was collected using the System Usability Scale (SUS) to evaluate the overall user experience. Performance testing measured the system’s responsiveness and accuracy, focusing on the time taken from image capture to the display of results, as well as the efficiency in processing high-resolution images and managing data. Together, these tests validated the system’s reliability, user-friendliness, and effectiveness in supporting early disease detection and timely intervention.

\subsection{Functional Testing}
Functional testing will be carried out to ensure that every part of the system works as intended. This testing will begin once the prototype is complete. It will focus on verifying key features, such as capturing images through the drone, detecting healthy and infected cacao pods using the YOLO, tagging infected trees’ locations with GPS, and displaying results clearly on the dashboard. Each function will be tested by giving specific inputs and checking if the outputs match what is expected. For example, the drone should respond correctly to manual and automatic commands, and the system should accurately identify pods and show their locations on the map. This testing helps find and fix any errors or missing functions, making sure the system is reliable and ready for real-world use.

\subsection{Usability Testing}
Usability testing will be conducted to make sure the system is easy and practical for cacao farmers to use. After the prototype is ready, farmers will be invited to try important features such as logging in, flying the drone, viewing pod detection results, and checking the map showing infected trees. After using the system, they will fill out a short survey called the System Usability Scale (SUS), which measures how user-friendly the system feels. The survey uses a rating scale from 1 to 5 and asks about how easy the system is to learn, how confident they feel using it, and whether the features work well together. Scores are converted to a total out of 100, with scores above 68 generally meaning the system is easy to use. This process will help the team understand what works well and what needs improvement before the system is fully deployed.

\subsection{Performance Testing}

Performance testing is conducted to evaluate the system’s responsiveness, accuracy, and overall reliability based on its core functionalities. This includes measuring the accuracy of detecting Phytophthora palmivora-infected cacao pods using the YOLO, assessing the precision of geolocation through GPS module and QGIS, and recording the system’s response time from image capture to the display of results on the dashboard. The test is carried out under standard operating conditions to determine whether the system can process high-resolution images efficiently and deliver real-time outputs. The results of this evaluation are essential in verifying that the system meets its intended performance criteria and is capable of supporting timely and informed decision-making in cacao disease management.
